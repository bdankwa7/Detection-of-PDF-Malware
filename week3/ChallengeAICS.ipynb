{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Challenge",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVRDAUgan_V5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install foolbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2CLOF-0rCsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "import numpy as np\n",
        "import foolbox\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBRf3y_ixHYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from time import perf_counter as pc\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "with open('/content/dict_train.json', 'r') as f:\n",
        "    raw_data = json.load(f)\n",
        "    print(raw_data.keys())\n",
        "\n",
        "labels = np.array(raw_data['labels'])\n",
        "N = len(labels)\n",
        "PCA_OUT_DIM = 32\n",
        "BATCH_SIZE = 32\n",
        "TRAIN_SIZE = int(N * 0.9)\n",
        "TEST_SIZE = N - TRAIN_SIZE\n",
        "NUM_TRAIN_BATCHES = int(TRAIN_SIZE/BATCH_SIZE) + int(TRAIN_SIZE%BATCH_SIZE != 0) * 1\n",
        "NUM_TEST_BATCHES = int(TEST_SIZE/BATCH_SIZE) + int(TEST_SIZE%BATCH_SIZE != 0) * 1\n",
        "\n",
        "\n",
        "\n",
        "def my_train_test_split(data_dict, reduce=True, reduce_dims=PCA_OUT_DIM):\n",
        "    #extract x\n",
        "    data = np.array(data_dict['data'])\n",
        "    row_index = np.array(data_dict['row_index'])\n",
        "    col_index = np.array(data_dict['column_index'])\n",
        "    \n",
        "    #extract y\n",
        "    labels = np.array(data_dict['labels'])\n",
        "    #use scipy sparse matrix\n",
        "    sparse = csr_matrix((data, (row_index, col_index)), shape=data_dict['shape'])\n",
        "    \n",
        "    #train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(sparse, labels, test_size=0.10, shuffle=False)\n",
        "    print(X_train.shape)\n",
        "    print(X_test.shape)\n",
        "    #apply PCA on sparse? (or dense) matrix\n",
        "    if reduce:\n",
        "        #toggle this with arg?\n",
        "        X_train_sparse = csr_matrix(X_train)\n",
        "        X_test_sparse = csr_matrix(X_test)\n",
        "        svd = TruncatedSVD(n_components=reduce_dims) #use truncatedSVD for sparse matrices\n",
        "        svd.fit(X_train_sparse) #only fit on TRAIN\n",
        "        #transform both train/test\n",
        "        X_train = svd.transform(X_train_sparse)\n",
        "        X_test = svd.transform(X_test_sparse)\n",
        "    return X_train, X_test, y_train, y_test \n",
        "\n",
        "   \n",
        "\n",
        "   \n",
        "\n",
        "\n",
        "#usage, where PCA_OUT_DIM is the number of features you want to reduce to.\n",
        "X_train, X_test, y_train, y_test = my_train_test_split(raw_data, reduce=True, reduce_dims=PCA_OUT_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h3IKyzTCDNGo",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  #tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OeOrNdnkEEcR",
        "colab": {}
      },
      "source": [
        "predictions = model(X_train[:1]).numpy()\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ2UuKxTBYXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.nn.softmax(predictions).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvzg9oAoBckM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVdeWCbSBgYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn(y_train[:1], predictions).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrMR73wTBmbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgXGe0o0CMqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(X_train, y_train, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F7dTAzgHDUh7",
        "colab": {}
      },
      "source": [
        "model.evaluate(X_test,  y_test, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PtupJqwGgFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "probability_model = tf.keras.Sequential([\n",
        "  model,\n",
        "  tf.keras.layers.Softmax()\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THc5oLhDIqtL",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cnqOZtUp1YR_",
        "colab": {}
      },
      "source": [
        "probability_model(X_test[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MLPobRGp9EC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fmodel = foolbox.models.TensorFlowModel(model, bounds=(0,100))\n",
        "#criterion = foolbox.criteria.Misclassification(labels)\n",
        "attack = foolbox.attacks.FGSM()\n",
        "epsilons = [0.0, 0.001, 0.01, 0.03, 0.1, 0.3, 0.5, 1.0]\n",
        "adversarial = attack(fmodel,X_train[:20],labels, epsilons)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}